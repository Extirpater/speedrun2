# Speedrun: Efficient Language Model Training

A high-performance framework for training and evaluating language models with advanced optimization techniques, including custom optimizers and quantization methods.

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10
- CUDA-compatible GPU (recommended: H100)
- Conda or pip

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/deepgrove-ai/speedrun.git
   cd speedrun
   ```

2. **Set up the environment:**
   ```bash
   # if conda is installed
   # conda create -n spd python=3.10
   # conda activate spd
   # else
   python -m venv env && source env/bin/activate
   bash setup.sh
   ```

3. **Prepare the dataset:**
   ```bash
   cd data
   bash process_data.sh # this command fails with (some regularity) if it does, just rerun the block
   cd ..
   ```

4. **Configure Weights & Biases (create an account if necessary):**
   ```bash
   wandb login
   ```

5. **Start training:**
   ```bash
   bash run.sh
   ```

6. **Evaluate the model:**
   ```bash
   bash eval.sh # alternative for 5 shot, run python eval.py --model_name <checkpoin> --tasks <task> --num_fewshot <num shots>
   ```

## ğŸ“ Project Structure

```
speedrun/
â”œâ”€â”€ alg/                          # Core training algorithms
â”‚   â”œâ”€â”€ args.py                   # Training configuration and arguments
â”‚   â”œâ”€â”€ run.py                    # Main training entry point
â”‚   â”œâ”€â”€ merge_trainer.py          # Custom trainer with advanced features
â”‚   â”œâ”€â”€ cadamw.py                 # Custom AdamW optimizer implementation
â”‚   â”œâ”€â”€ muon.py                   # Muon optimizer implementation
â”‚   â”œâ”€â”€ models.py                 # Model definitions and utilities
â”‚   â”œâ”€â”€ data.py                   # Data loading and preprocessing
â”‚   â”œâ”€â”€ metrics.py                # Training metrics and evaluation
â”‚   â””â”€â”€ objectives/               # Loss functions and training objectives
â”‚       â”œâ”€â”€ loss.py               # Core loss functions
â”‚       â”œâ”€â”€ objectives.py         # Training objective implementations
â”‚       â”œâ”€â”€ projectors.py         # Feature projection layers (can be mostly ignored)
â”‚       â”œâ”€â”€ whiten.py             # Whitening transformations (can be mostly ignored)
â”‚       â”œâ”€â”€ norm.py               # Normalization utilities (can be mostly ignored)
â”‚       â””â”€â”€ layer_mappers.py      # Layer mapping utilities (can be mostly ignored)
â”œâ”€â”€ data/                         # Data processing and configuration
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ data.json             # Dataset configuration
â”‚   â”œâ”€â”€ scripts/                  # Data processing scripts
â”‚   â””â”€â”€ process_data.sh           # Data preparation script
â”œâ”€â”€ eval.py                       # Model evaluation script
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ setup.sh                      # Environment setup script
â”œâ”€â”€ run.sh                        # Training script
â””â”€â”€ eval.sh                       # Evaluation script
```

## âš™ï¸ Configuration

### Training Parameters

Key training parameters can be modified in `alg/args.py`:

- **Model**: `model_name` - HuggingFace model identifier
- **Optimizer**: `optimizer_name` - Choice of optimizer (cadamw, muon, etc.)
- **Batch Size**: `per_device_train_batch_size` - Training batch size per device
- **Learning Rate**: `learning_rate` - Initial learning rate
- **Max Steps**: `max_steps` - Total training steps (default is 16,000)

### Dataset Configuration

Modify `data/configs/data.json` to configure your training datasets:

```json
{
  "datasets": [
    { "path": "dataset_name", "limit": 0.51, "num_valid_samples": 8 },
    // Add more datasets...
  ],
  "concat_tokens": 4096,
  "tokenizer": "kaizen9/test2b12"
}
```

Each dataset entry includes:
- `path`: HuggingFace dataset identifier
- `limit`: Fraction of total tokens (0.0-1.0)
- `num_valid_samples`: Number of validation samples

## ğŸ”§ Key Features

### Optimizers
- **cadamw**: Custom AdamW implementation with enhanced features
- **MuonWithAuxAdam**: Novel optimizer with auxiliary parameters
- Note that no other optimizers are currently supported

## ğŸ“Š Evaluation

The framework includes comprehensive evaluation on standard benchmarks:

- **MMLU**: Massive Multitask Language Understanding
- **HellaSwag**: Commonsense reasoning
- **ARC Challenge**: AI2 Reasoning Challenge
- **Custom Tasks**: Extensible evaluation framework

### Running Evaluations

```bash
# Evaluate on all default tasks
python eval.py --model_name path/to/model

# Evaluate on specific tasks
python eval.py --model_name path/to/model --tasks mmlu hellaswag

# Few-shot evaluation
python eval.py --model_name path/to/model --num_fewshot 5
```

## ğŸ¯ Performance

Baseline results achieved with default configuration:

| Benchmark | 0-shot | 5-shot |
|-----------|--------|--------|
| MMLU (acc) | 44.08% | 46.30% |
| HellaSwag (acc norm) | 51.99% | - |
| ARC Challenge (acc norm) | 36.00% | - |


## ğŸ” Troubleshooting

### Common Issues

1. **CUDA Out of Memory**: Reduce `per_device_train_batch_size` or enable gradient checkpointing
2. **Data Loading Errors**: Decrease `num_workers` in data processing scripts
3. **Slow Training**: Enable `torch_compile` and ensure proper GPU utilization

### Performance Tips

- Use `bf16` precision for better performance on modern GPUs
- Enable gradient checkpointing for memory efficiency
- Use appropriate batch sizes for your hardware
- Monitor GPU utilization and adjust accordingly
