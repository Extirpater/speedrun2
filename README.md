# Speedrun: Efficient Language Model Training

A high-performance framework for training and evaluating language models with advanced optimization techniques, including custom optimizers and quantization methods.

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10
- CUDA-compatible GPU (recommended: H100)
- Conda or pip

### Installation

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd speedrun
   ```

2. **Set up the environment:**
   ```bash
   bash setup.sh
   ```

3. **Prepare the dataset:**
   ```bash
   cd data
   bash process_data.sh
   cd ..
   ```

4. **Configure Weights & Biases (optional but recommended):**
   ```bash
   wandb login
   ```

5. **Start training:**
   ```bash
   bash run.sh
   ```

6. **Evaluate the model:**
   ```bash
   bash eval.sh
   ```

## ğŸ“ Project Structure

```
speedrun/
â”œâ”€â”€ alg/                          # Core training algorithms
â”‚   â”œâ”€â”€ args.py                   # Training configuration and arguments
â”‚   â”œâ”€â”€ run.py                    # Main training entry point
â”‚   â”œâ”€â”€ merge_trainer.py          # Custom trainer with advanced features
â”‚   â”œâ”€â”€ cadamw.py                 # Custom AdamW optimizer implementation
â”‚   â”œâ”€â”€ muon.py                   # Muon optimizer implementation
â”‚   â”œâ”€â”€ models.py                 # Model definitions and utilities
â”‚   â”œâ”€â”€ data.py                   # Data loading and preprocessing
â”‚   â”œâ”€â”€ metrics.py                # Training metrics and evaluation
â”‚   â””â”€â”€ objectives/               # Loss functions and training objectives
â”‚       â”œâ”€â”€ loss.py               # Core loss functions
â”‚       â”œâ”€â”€ objectives.py         # Training objective implementations
â”‚       â”œâ”€â”€ projectors.py         # Feature projection layers
â”‚       â”œâ”€â”€ whiten.py             # Whitening transformations
â”‚       â”œâ”€â”€ norm.py               # Normalization utilities
â”‚       â””â”€â”€ layer_mappers.py      # Layer mapping utilities
â”œâ”€â”€ data/                         # Data processing and configuration
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ data.json             # Dataset configuration
â”‚   â”œâ”€â”€ scripts/                  # Data processing scripts
â”‚   â””â”€â”€ process_data.sh           # Data preparation script
â”œâ”€â”€ eval.py                       # Model evaluation script
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ setup.sh                      # Environment setup script
â”œâ”€â”€ run.sh                        # Training script
â””â”€â”€ eval.sh                       # Evaluation script
```

## âš™ï¸ Configuration

### Training Parameters

Key training parameters can be modified in `alg/args.py`:

- **Model**: `model_name` - HuggingFace model identifier
- **Optimizer**: `optimizer_name` - Choice of optimizer (cadamw, muon, etc.)
- **Batch Size**: `per_device_train_batch_size` - Training batch size per device
- **Learning Rate**: `learning_rate` - Initial learning rate
- **Max Steps**: `max_steps` - Total training steps
- **Context Length**: `concat_tokens` - Sequence length for training

### Dataset Configuration

Modify `data/configs/data.json` to configure your training datasets:

```json
{
  "datasets": [
    { "path": "dataset_name", "limit": 0.51, "num_valid_samples": 8 },
    // Add more datasets...
  ],
  "concat_tokens": 4096,
  "tokenizer": "kaizen9/test2b12"
}
```

Each dataset entry includes:
- `path`: HuggingFace dataset identifier
- `limit`: Fraction of total tokens (0.0-1.0)
- `num_valid_samples`: Number of validation samples

## ğŸ”§ Key Features

### Advanced Optimizers
- **CaAdamW**: Custom AdamW implementation with enhanced features
- **Muon**: Novel optimizer with auxiliary parameters
- **Liger Kernel**: Optional kernel optimization for performance

### Quantization Support
- **TWN Quantization**: Ternary Weight Networks for model compression
- **Flash Attention**: Optimized attention implementation
- **Gradient Checkpointing**: Memory-efficient training

### Training Features
- **Knowledge Distillation**: Student-teacher training paradigm
- **Multi-GPU Support**: Distributed training with Accelerate
- **Weights & Biases Integration**: Comprehensive experiment tracking
- **Gradient Statistics**: Advanced gradient analysis and monitoring

## ğŸ“Š Evaluation

The framework includes comprehensive evaluation on standard benchmarks:

- **MMLU**: Massive Multitask Language Understanding
- **HellaSwag**: Commonsense reasoning
- **ARC Challenge**: AI2 Reasoning Challenge
- **Custom Tasks**: Extensible evaluation framework

### Running Evaluations

```bash
# Evaluate on all default tasks
python eval.py --model_name path/to/model

# Evaluate on specific tasks
python eval.py --model_name path/to/model --tasks mmlu hellaswag

# Few-shot evaluation
python eval.py --model_name path/to/model --num_fewshot 5
```

## ğŸ¯ Performance

Baseline results achieved with default configuration:

| Benchmark | 0-shot | 5-shot |
|-----------|--------|--------|
| MMLU | 44.08% | 46.30% |
| HellaSwag | 51.99% | - |
| ARC Challenge | 36.00% | - |

## ğŸ› ï¸ Advanced Usage

### Custom Training

```python
from alg.run import train
from alg.args import get_args

# Get default arguments
training_args, model_args, dataset_args, eval_args = get_args()

# Modify arguments as needed
training_args.learning_rate = 2e-4
training_args.max_steps = 20000

# Start training
train(training_args, model_args, dataset_args, eval_args)
```

### Benchmarking

```python
from alg.run import benchmark

# Run hyperparameter sweep
benchmark(
    learning_rate=[1e-4, 2e-4, 4e-4],
    optimizer_name=["cadamw", "muon"],
    per_device_train_batch_size=[2, 4, 8]
)
```

## ğŸ” Troubleshooting

### Common Issues

1. **CUDA Out of Memory**: Reduce `per_device_train_batch_size` or enable gradient checkpointing
2. **Data Loading Errors**: Decrease `num_workers` in data processing scripts
3. **Slow Training**: Enable `torch_compile` and ensure proper GPU utilization

### Performance Tips

- Use `bf16` precision for better performance on modern GPUs
- Enable gradient checkpointing for memory efficiency
- Use appropriate batch sizes for your hardware
- Monitor GPU utilization and adjust accordingly

## ğŸ“ License

[Add your license information here]

## ğŸ¤ Contributing

[Add contribution guidelines here]

## ğŸ“š References

- [AdamW Paper](https://arxiv.org/abs/1711.05101)
- [Flash Attention](https://arxiv.org/abs/2205.14135)
- [Knowledge Distillation](https://arxiv.org/abs/1503.02531)